{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# L1 and L2 Regularization\n\nThis tutorial shows how to incorporate regularization into the\n:class:`~skfolio.optimization.MeanRisk` optimization.\n\nRegularization tends to increase robustness and out-of-sample stability.\n\nThe `l1_coef` parameter is used to penalize the objective function by the L1 norm:\n\n\\begin{align}l1\\_coef \\times \\Vert w \\Vert_{1} = l1\\_coef \\times \\sum_{i=1}^{N} |w_{i}|\\end{align}\n\nand the `l2_coef` parameter is used to penalize the objective function by the L2 norm:\n\n\\begin{align}l2\\_coef \\times \\Vert w \\Vert_{2}^{2} = l2\\_coef \\times \\sum_{i=1}^{N} w_{i}^2\\end{align}\n\n.. warning ::\n\n    Increasing the L1 coefficient may reduce the number of non-zero weights\n    (cardinality), which can reduce diversification. However, a reduction in\n    diversification does not necessarily equate to a reduction in robustness.\n\n.. note ::\n\n    Increasing the L1 coefficient has no impact if the portfolio is long only.\n\nIn this example we will use a dataset with a large number of assets and long-short\nallocation to exacerbate overfitting.\n\nFirst, we will analyze the impact of regularization on the entire Mean-Variance efficient\nfrontier and its stability from the training set to the test set. Then, we will show how\nto tune the regularization coefficients using cross-validation with `GridSearchCV`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data\nWe load the FTSE 100 `dataset <datasets>` composed of the daily prices of 64\nassets from the FTSE 100 Index composition starting from 2000-01-04 up to 2023-05-31.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport plotly.graph_objects as go\nfrom plotly.io import show\nfrom scipy.stats import loguniform\nfrom sklearn import clone\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n\nfrom skfolio import PerfMeasure, Population, RatioMeasure, RiskMeasure\nfrom skfolio.datasets import load_ftse100_dataset\nfrom skfolio.metrics import make_scorer\nfrom skfolio.model_selection import WalkForward, cross_val_predict\nfrom skfolio.optimization import EqualWeighted, MeanRisk, ObjectiveFunction\nfrom skfolio.preprocessing import prices_to_returns\n\nprices = load_ftse100_dataset()\nX = prices_to_returns(prices)\n\nX_train, X_test = train_test_split(X, test_size=0.33, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Efficient Frontier\nFirst, we create a Mean-Variance model to estimate the efficient frontier without\nregularization. We constrain the volatility to be below 30% p.a.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = MeanRisk(\n    risk_measure=RiskMeasure.VARIANCE,\n    min_weights=-1,\n    max_variance=0.3**2 / 252,\n    efficient_frontier_size=30,\n    portfolio_params=dict(name=\"Mean-Variance\", tag=\"No Regularization\"),\n)\nmodel.fit(X_train)\nmodel.weights_.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the two regularized models:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_l1 = MeanRisk(\n    risk_measure=RiskMeasure.VARIANCE,\n    min_weights=-1,\n    max_variance=0.3**2 / 252,\n    efficient_frontier_size=30,\n    l1_coef=0.001,\n    portfolio_params=dict(name=\"Mean-Variance\", tag=\"L1 Regularization\"),\n)\nmodel_l1.fit(X_train)\n\nmodel_l2 = clone(model_l1)\nmodel_l2.set_params(\n    l1_coef=0,\n    l2_coef=0.001,\n    portfolio_params=dict(name=\"Mean-Variance\", tag=\"L2 Regularization\"),\n)\nmodel_l2.fit(X_train)\nmodel_l2.weights_.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the efficient frontiers on the training set:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "population_train = (\n    model.predict(X_train) + model_l1.predict(X_train) + model_l2.predict(X_train)\n)\n\npopulation_train.plot_measures(\n    x=RiskMeasure.ANNUALIZED_STANDARD_DEVIATION,\n    y=PerfMeasure.ANNUALIZED_MEAN,\n    color_scale=RatioMeasure.ANNUALIZED_SHARPE_RATIO,\n    hover_measures=[RiskMeasure.MAX_DRAWDOWN, RatioMeasure.ANNUALIZED_SORTINO_RATIO],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction\nThe parameter `efficient_frontier_size=30` means that when we called the `fit` method,\neach model ran 30 optimizations along the efficient frontier. Therefore, the `predict`\nmethod will return a :class:`~skfolio.population.Population` composed of 30\n:class:`~skfolio.portfolio.Portfolio`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "population_test = (\n    model.predict(X_test) + model_l1.predict(X_test) + model_l2.predict(X_test)\n)\n\nfor tag in [\"No Regularization\", \"L1 Regularization\"]:\n    print(\"=================\")\n    print(tag)\n    print(\"=================\")\n    print(\n        \"Avg Sharpe Ratio Train:\"\n        f\" {population_train.measures_mean(measure=RatioMeasure.ANNUALIZED_SHARPE_RATIO, tags=tag):0.2f}\"\n    )\n    print(\n        \"Avg Sharpe Ratio Test:\"\n        f\" {population_test.measures_mean(measure=RatioMeasure.ANNUALIZED_SHARPE_RATIO, tags=tag):0.2f}\"\n    )\n    print(\n        \"Avg non-zeros assets:\"\n        f\" {np.mean([len(ptf.nonzero_assets) for ptf in population_train.filter(tags=tag)]):0.2f}\"\n    )\n    print(\"\\n\")\n\npopulation_test.plot_measures(\n    x=RiskMeasure.ANNUALIZED_STANDARD_DEVIATION,\n    y=PerfMeasure.ANNUALIZED_MEAN,\n    color_scale=RatioMeasure.ANNUALIZED_SHARPE_RATIO,\n    hover_measures=[RiskMeasure.MAX_DRAWDOWN, RatioMeasure.ANNUALIZED_SORTINO_RATIO],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we can clearly see that L1 regularization reduced the number of assets\n(from 64 down to 14) and made the model more robust: the portfolios without\nregularization have a higher Sharpe on the train set and a lower Sharpe on the test\nset compared to the portfolios with regularization.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyper-parameter Tuning\nIn this section, we consider a 3 months rolling (60 business days) long-short\nallocation fitted on the preceding year of data (252 business days) that maximizes the\nreturn under a volatility constraint of 30% p.a.\n\nWe use `GridSearchCV` to select the optimal L1 and L2 regularization coefficients on\nthe training set using cross-validation that achieve the highest\nmean test score. We use the default score, which is the Sharpe ratio.\nFinally, we evaluate the model on the test set and compare it with the equal-weighted\nbenchmark and a reference model without regularization:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ref_model = MeanRisk(\n    risk_measure=RiskMeasure.VARIANCE,\n    objective_function=ObjectiveFunction.MAXIMIZE_RETURN,\n    max_variance=0.3**2 / 252,\n    min_weights=-1,\n)\n\ncv = WalkForward(train_size=252, test_size=60)\n\ngrid_search = GridSearchCV(\n    estimator=ref_model,\n    cv=cv,\n    n_jobs=-1,\n    param_grid={\n        \"l1_coef\": [0.001, 0.01, 0.1],\n        \"l2_coef\": [0.001, 0.01, 0.1],\n    },\n)\ngrid_search.fit(X_train)\nbest_model = grid_search.best_estimator_\nprint(best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The optimal parameters among the above 3x3 grid are 0.01 for the L1 coefficient\nand the L2 coefficient.\nThese parameters are the ones that achieved the highest mean out-of-sample Sharpe\nRatio. Note that the score can be changed to another measure or function using the\n`scoring` parameter.\n\nFor continuous parameters, such as L1 and L2 above, a better approach is to use\n`RandomizedSearchCV` and specify a continuous distribution to take full advantage of\nthe randomization.\n\nA continuous log-uniform random variable is the continuous version of a log-spaced\nparameter. For example, to specify the equivalent of the L1 parameter from above,\n`loguniform(1e-3, 1e-1)` can be used instead of `[0.001, 0.01, 0.1]`.\n\nMirroring the example above in grid search, we can specify a continuous random\nvariable that is log-uniformly distributed between 1e-3 and 1e-1:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "randomized_search = RandomizedSearchCV(\n    estimator=ref_model,\n    cv=cv,\n    n_jobs=-1,\n    param_distributions={\n        \"l2_coef\": loguniform(1e-3, 1e-1),\n    },\n    n_iter=100,\n    return_train_score=True,\n    scoring=make_scorer(RatioMeasure.ANNUALIZED_SHARPE_RATIO),\n)\nrandomized_search.fit(X_train)\nbest_model_rd = randomized_search.best_estimator_\nprint(best_model_rd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot both the average in-sample and out-of-sample scores (annualized Sharpe\nratio) as a function of `l2_coef`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cv_results = randomized_search.cv_results_\nx = np.asarray(cv_results[\"param_l2_coef\"]).astype(float)\nsort_idx = np.argsort(x)\ny_train_mean = cv_results[\"mean_train_score\"][sort_idx]\ny_train_std = cv_results[\"std_train_score\"][sort_idx]\ny_test_mean = cv_results[\"mean_test_score\"][sort_idx]\ny_test_std = cv_results[\"std_test_score\"][sort_idx]\nx = x[sort_idx]\n\nfig = go.Figure([\n    go.Scatter(\n        x=x,\n        y=y_train_mean,\n        name=\"Train\",\n        mode=\"lines\",\n        line=dict(color=\"rgb(31, 119, 180)\"),\n    ),\n    go.Scatter(\n        x=x,\n        y=y_train_mean + y_train_std,\n        mode=\"lines\",\n        line=dict(width=0),\n        showlegend=False,\n    ),\n    go.Scatter(\n        x=x,\n        y=y_train_mean - y_train_std,\n        mode=\"lines\",\n        line=dict(width=0),\n        showlegend=False,\n        fillcolor=\"rgba(31, 119, 180,0.15)\",\n        fill=\"tonexty\",\n    ),\n    go.Scatter(\n        x=x,\n        y=y_test_mean,\n        name=\"Test\",\n        mode=\"lines\",\n        line=dict(color=\"rgb(255,165,0)\"),\n    ),\n    go.Scatter(\n        x=x,\n        y=y_test_mean + y_test_std,\n        mode=\"lines\",\n        line=dict(width=0),\n        showlegend=False,\n    ),\n    go.Scatter(\n        x=x,\n        y=y_test_mean - y_test_std,\n        line=dict(width=0),\n        mode=\"lines\",\n        fillcolor=\"rgba(255,165,0, 0.15)\",\n        fill=\"tonexty\",\n        showlegend=False,\n    ),\n])\nfig.add_vline(\n    x=randomized_search.best_params_[\"l2_coef\"],\n    line_width=2,\n    line_dash=\"dash\",\n    line_color=\"green\",\n)\nfig.update_layout(\n    title=\"Train/Test score\",\n    xaxis_title=\"L2 Coef\",\n    yaxis_title=\"Annualized Sharpe Ratio\",\n)\nfig.update_yaxes(tickformat=\".2f\")\nshow(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|\n\nThe highest mean out-of-sample Sharpe Ratio is 1.55 and is achieved for a L2 coef of\n0.023.\nAlso note that without regularization, the mean train Sharpe Ratio is around\nsix time higher than the mean test Sharpe Ratio. That would be a clear indiction of\noverfitting.\n\nNow, we analyze all three models on the test set. By using `cross_val_predict` with\n`WalkForward`, we are able to compute efficiently the `MultiPeriodPortfolio`\ncomposed of 60 days rolling portfolios fitted on the preceding 252 days:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmark = EqualWeighted()\npred_bench = cross_val_predict(benchmark, X_test, cv=cv)\npred_bench.name = \"Benchmark\"\n\npred_no_reg = cross_val_predict(ref_model, X_test, cv=cv)\npred_no_reg.name = \"No Regularization\"\n\npred_reg = cross_val_predict(best_model, X_test, cv=cv, n_jobs=-1)\npred_reg.name = \"Regularization\"\n\npopulation = Population([pred_no_reg, pred_reg, pred_bench])\npopulation.plot_cumulative_returns()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the plot and the below summary, we can see that the un-regularized model is\noverfitted and perform poorly on the test set. Its annualized volatility is 54%, which\nis significantly above the model upper-bound of 30% and its Sharpe Ratio is 0.32 which\nis the lowest of all models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "population.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we plot the composition of the regularized multi-period portfolio:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pred_reg.plot_composition()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}